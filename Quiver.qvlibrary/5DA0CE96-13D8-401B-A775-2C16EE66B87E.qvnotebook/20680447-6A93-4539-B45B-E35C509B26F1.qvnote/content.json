{
  "title": "13 November // Lecture 24",
  "cells": [
    {
      "type": "markdown",
      "data": "### Multiplication (cont.)\nReview of iterative performance gains\nPositives easy, for negatives see book\nWe also saw the array multiplier (iterating in space instead of time) (takes about $O(n^2)$ and gates)\nThe longest path has propagation delay $3nt$\n\n## Division\nThe algorithm is the same as long division by hand\nThink division with remainder from discrete\n\n# Floating Point Arithmetic\nWe need these\n\n## Scientific Notation $\\checkmark$\nNormalized\n\nRepresentation easy, arithmetic hard\nAddition in pieces\nMultiplication in pieces\n\nIn binary, it gets easier\n\n### Floating Point Represenatation\n`S`: Sign\n`E`: Exponent + 127 (127-biased 8-bit integer)\n`F`: Significand/Mantissa/&c. (unsigned fixed-point with hidden leading '1' due to normalization)\n\nValue: $N = {-1}^S \\times (1 + F) \\times 2^{E - 127}$\n\nExamples:\n\n- **$1$**\n$S = 0, E = 0 + 127, F = 1.0 - 1$\n`0 01111111 0000...` = `0x3f800000`\n\n- **$\\frac{1}{2}$**\nExponent changes\n`0 01111110 0000...` = `0x3f000000`\n\n- **$-2$**\nSign changes\n`0 10000000 0000...` = `0xc0000000`\n\nOrder of representation for ease of comparing\n\n### What about 0?\nThere is a smallest number we can generate, and it's not 0\n\nSo IEEE convention gave us this:\n```python\nif E == 0:\n    pass # intepret number differently\n```\n\nWe use the significand `0` to represent 0, and we get a $\\pm 0$ depending on `S`\n\nPositive Infinity:\n`S = 0, E = 255, F = 0`\n\nNegative Infinity\n`S = 1, E = 255, F = 0`\n\nNAN\n`S = ?, E = 255, F != 0`\n$\\sqrt{-1}, - \\infty \\times 42, \\frac{0}{0}, \\frac{\\infty}{\\infty}, \\log(-5)$\n\nHowever:\n$\\frac{1}{0} = \\infty$\n\n$\\frac{-1}{0} = - \\infty$\n\n$\\log(0) = - \\infty$\n\n### Low-End of Spectrum\nThere is a denormalized gap where we can't get closer to 0 than $2^{-127}$\n\nWe fill it with denormalized numbers\nWhen the `E` field is 0, we no longer consider there to be a hidden 1\nWe consider the exponent to be $-126$\n\n### Couple of other \"Tricks\"\nFloating point arithmetic is NOT your usual arithmetic\n$1.0 \\neq 10 \\times 0.1$ due to limited precision\n`0.1` in binary is a repeated fractional series\n\nAdding order can make a difference\n\nSometimes, `x + 1 == x`\n\nYou never compare for equality in floating point; you decide whether or not their difference is smaller than some acceptable epsilon (or delta)"
    }
  ]
}