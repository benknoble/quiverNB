{
  "title": "26 April // Last Lecture",
  "cells": [
    {
      "type": "markdown",
      "data": "Take the survey on the website\n\nWill be looking at some rarer-seen data structures, why they're important, and end with PollEv to review them\n\n**Example**\nGoogle search: predictive suggestions\n\nData structure that will let you do it, esp. efficiently\n\nBasic Idea: *Trie* (pronounced \"try\")\n\nDo not encode each *word* in a node: the words come from the *pathways*\n\n### Complexity? Cost? Why are we talking about this?\n\nThink differently... what if $n$ is the length of the word? Different than $n$ being number of nodes.\nSo usage is $O(n)$ in time complexity... but in space (memory usage), the trie grows factorially fast (actually, even larger)\n$O(n^w)$ in memory where $n$ is the number of possible characters and $w$ is the length of the longest word\n\n$n! \\gg 2^n$\n\nCost of building is so much more than using it\n\n## Skip Lists\n\nGeneralization of a sorted linked list\nAlternative to balanced BST\nProbabilistic Data Structure\n- we roll dice/flip coins to build the structure\n\n#### First, 'perfect' skip list (no dice)\nSorted linked list AND elements maintain pointers to next element, 2nd next, 3rd next, sometimes more\nAllows you to skip items (hence skip list)\nMultiple lists, multi-level list $O(\\log n)$ levels... it's a perfect binary search tree, even though it doesn't look like it!\n\n`find`\nLook as far ahead as possible: 7 smaller or bigger?\nSmaller, drop down. repeat\nBST performance\n\n#### A skip list is *almost* perfect, less complex than AVL tree\nEvery new item must be in a cell of variable size (how far ahead can I skip)\nLet probability determine the sizes of new cells\nAs long as uniform-ish, well-distributed, we get good average behavior (works best for large lists, so that the probabilities work nicely for us)\n\nProbability is such that there are twice as many at each successive level (uses a flip repeater, e.g., flips a coin until you get heads)\n\n#### Complexity\nWorst-case is they're all size 1 cells, so $O(n)$... not too happy about that\nHowever, this is incredibly unlikely *especially in long term behavior with lots of elements*! So, almost always we get good $O(\\log n)$ average case\n\nWay way faster than $O(n^2)$\n\n# Poll Ev\n\n> Q: Consider a skip list with $n$ items. The worst case time complexity of finding an element is\n> A: $O(n)$ worst case but infinitesimally rare\n\n> Q: Consider a skip list with $n$ items. The average case time complexity of inserting a new element is\n> A: $O(\\log n)$\n\n> Q: Consider a skip list with $n$ items where $n$ is large... let's say $n \\gt 1,000,000$. If we do a find operation for an item, how likely is it that the operation will take the worst case times complexity?\n> A: Nearly impossible\n\n> Q: Consider a trie with depth $n$. The worst case time complexity of using the trie to find an item is\n> A: $O(n)$ (depth is the longest word...)\n\n> Q: Consider a trie with depth $n$. At each node we have $k$ possible next nodes. The worst case space complexity of the trie (# of nodes) is\n> A: $O(k^n)$\n\n> Q: Why do we care about worst case space complexity ?\n> A: Might have a structure that we could use efficiently if handed to us but impractical to build. Also tells us about worst case time, how much memory we need, that sort of thing."
    }
  ]
}