{
  "title": "20 February // Lecture 11",
  "cells": [
    {
      "type": "markdown",
      "data": "PollEverywhere\n\n> Q: The term \"garbage collection\" is a procedure in the run-time system of a language whereby\n> A: dynamic memory allocated in the heap, but no longer reachable from a program, is returned to the un-allocated heap\n\n> Q: Consider implementing a Priority Queue (PrQUE) with a normal Linked Cell List. What is the worst case time complexity of the \"enq\" operation for a PrQUE with N items in it?\n> A: Need to make a choice: $O(1)$ enq and $O(n)$ deq OR $O(n)$ enq and $O(1)$ deq. Pick based on which operation you think you will be doing more of. Because this is an unsorted implementation, we choose a cheap enq because deq will always be expensive. $O(1)$ is the best worst case.\n\n> Q: Consider implementing a Priority Queue (PrQUE) with a normal Linked Cell List. What is the worst case time complexity of the \"front\" operation for a PrQUE with N items in it? (remember, \"front\" gives the highest priority element).\n> A: Can get $O(1)$ using a sorted linked list, but unsorted requires $O(n)$. Technically, the question does not present a sorted structure.\n\nAside: we could use a binary search tree to get average insert $O(\\log n)$, average front $O(\\log n)$. But the worst case is still $O(n)$ for both.\n\n#Priority Queue\n###Implementation (cont.)\nIs there another way to get an efficient priority queue?\n\nYes! We give up some information in order to find the minimum element more efficiently using *Binary Heaps*. No, it is not related to compiler heaps. Those are different.\nBST is actually overkill.\n\n###Binary Heap\n2 Properties:\n1. Structure Property: Complete binary tree (excepting leaf nodes--those we fill from left to right)\n2. Heap-order Property: Minimum element is at the root; every child $\\ge$ parent; each path is an ordered list\n\nSee ppt for images\n\nFor Priority Queue, we use the priority fields to build the heap using it's heap order property. Obviously, it must also contain actual object data, like a process to execute\n\nWe can also see that every subtree of a binary heap is a binary heap, so we can define them recursively.\n\n**Implemenation**\nWe can do this efficiently with an array\n- Slot $0$ goes unused\n- Store node val as array element\n- for node in slot $i$\n  - parent is at $\\left\\lfloor{\\frac i 2}\\right\\rfloor$\n  - left child is $2i$\n  - right child is $2i + 1$\n\n**Insert**\nSomething has to be inserted at the last empty slot, but we can't throw it in there without possibly violating the heap order\nWe have to double check the order: if we violate, we get to do some swaps. Swap with parent until order is intact, or inserted element is the root\nAt most, $O(\\log n)$ swaps. That's good insert\n\nDifference between bubble element and bubble up hole:\n- 3 swaps per bubble of element is $O(3 \\log n) = O(\\log n)$\n- Pull parents down into hole and then insert the element; this is $O(\\log n)$ but saves $\\frac{1}{3}$ of the work\n\n###Complexity of Binary Heap\n- insert $O(\\log n)$ *worst case*\n- getMin $O(1)$\n- delMin... well it gets complicated\n\n**Remove Min**\n- Remove root value\n- Save the last element in the array in `temp`\n- Pick the smallest of the children to move up (moves hole down) (keeps heap order)\n- When we've found a spot that maintains heap order, we insert `temp`\n\nso delMin is $O(\\log n)$\n\nThis has given us a lot of efficiency at the cost of total-ordering of a BST\n\n###Limits\nCan't *directly* get a full sort out of a binary heap\nTakes $O(n + n\\log n) = O(n)$ to traverse a BST to get a sort (average behavior; worst is $O(n^2)$\nHeapsort is very efficient, but how? and how efficient?\n\nWell, we can build a heap and repeatedly delete the min (keeping track of it in our sorted list)\n$O(n\\log n)$ to build, and $O(n\\log n)$ to destruct, so we get $O(2n\\log n) = O(n\\log n)$\nTurns out there's an even more efficient way to build the heap that is $O(n)$ provided we have all $n$ numbers"
    }
  ]
}