{
  "title": "30 January // Lecture 5",
  "cells": [
    {
      "type": "markdown",
      "data": "1. PollEverywhere\n\n>Q: Consider a LIST implemented by linked cells, and containing N elements. The get operation has worst case time complexity of...\n>\n>A: $O(N)$, *i.e.* it's a linear-time search\n\n>Q: Consider a LIST implemented by array, and containing N elements. The get operation has worst case time complexity of...\n>\n>A: $O(1)$, *i.e.* it takes constant time (to index an array)\n\n>Q: Consider a STACK implemented with an array and containing N elements. The push operation has worst case time complexity of...\n>\n>A: $O(N)$, because in the worst case you have to expand the array, or if the Stack is implemented with the top at slot 0, such that every push requires shifting the array. Can get $O(1)$ (**correct answer**) most of the time, using the end of the array as the top.\n\n>Q: Consider a STACK implemented with linked cells, and containing N elements. The push operation has worst case time complexity of...\n>\n>A: $O(1)$, if you add to the beginning of the linked list. Inserting at the first position is cheap in a linked list provided you have easy access to the first element.\n\n>Q: Consider a QUEUE implemented by array, and containing N elements. The deque operation has worst case time complexity of...\n>\n>A: $O(1)$, as dequeueing never requires resizing the array and simply requires an array access and size decrement. Also get $O(N)$. Two ways to implement Queue with array: give $O(1)$ and $O(N)$ for enqueue and dequeue respectively, or vice-versa\n\n>Q: Consider a QUEUE implemented by linked list, and containing N elements. If we manage to implement the enque operation in $O(1)$ time worst case, what is the \"best\" worst case time complexity we can get for the deque operation?\n>\n>A: $O(1)$, provided you maintain both a `head` and a `tail` reference\n\n#Complexity, or \"Big-Oh\"\n##Basic Algorithmic Complexity\n\n###Complexity\n- Analyze \"betterness\" (usefullness, time-complexity efficiency, &c.) of algorithms\n- Want general categories that are similar in application to practical problems\n- Yet, allow \"unimportant\" differences betwen different algorithms in a category\n\n###\"Big Oh\"\n- Notation called \"Big Oh\"\n- Describes how fast the run-time grows as the size of the problem grows (ignores external factors like hardware speed)\n- Problem size is usually the amount of data to be processed (length of list, number or items to sort, &c.)\n\n(There are some good graphs in the ppt)\n\n$O(N)$: Linear time complexity: As the problem grows, the run-time grows proportionately\n\n$O(N^2)$: Quadratic time complexity: As the problem grows, the run-time grows proportionately to it's square; thus, the run-time increases faster for large problems than for $O(N)$\n\nSuppose the timing takes exactly twice the size of the problem ($2N$)? It's still linear, so $O(N)$! It grows at the same rate, despite the time being longer\n\n*Math*\n**Upper bounds (big-Oh)** $T(n) = O(f(n))$ if there are positive constants $c$ and $n_0$ such that $T(n) \\le cf(n)$ when $n \\ge n_0$\nIn other words, the time complexity $T(n)$ of an algorithm is always less than $O(f(n))$; it is an **upper bound**\nMeasuring the *shape* of the curve\n\n###Categories\n$O(1)$ constant\n$O(\\log N)$ logarithmic\n$O(\\log^2 N)$ log-squared\n$O(\\sqrt N)$ square root\n$O(N)$ linear\n$O(N \\log N)$ log-linear\n$O(N^2)$ quadratic\n$O(N^3)$ cubic\n$O(2^N)$ exponential\n(To infinity, and beyond!)\n\n(See links in ppt for more explanation)\n\n####For Loop\n- If the loop body is $O(1)$ and the loop goes worst-case $n$ times, the loop is $O(n)$ in total\n- If the loop body takes $O(n^2)$ and the loop goes worst-case $n$ times, the loop is $O(n\\times n^2) = O(n^3)$\n\n####Consecutive Statements\n- Add $O$ of each statment, so growth rate is dominated by worst cases of individuals\n*e.g.*, $O(n^2 + n) = O(n^2)$\n\n####If-Else\n- Compute $O$ of both blocks, and take the worst\n\n####Recursion\n- Very powerful, very succint\n- Often, code directly reflects math formula\n- (Big-oh slightly harder to calculate unless you convert to a loop)\n- Consider `fibonacci(n)`: The typical recursive solution contains two recursive calls, which is incredibly easy to write but incredibly inefficient for large integers\n- `fibonacci(n)` has time-complexity $O(\\frac {2^n}{2}) = O(2^{n-1}) = O(2^n)$ -- inefficiency shows up because already computed work is redone\n- Can be better done either with a cache (save the results) *or* using a while-loop $O(n)$\n\n#Trees (not the wooden ones)\n##$n$-ary tree, binary tree\n\nThoughts... LinkedLists are linear; each cell has a single \"next\" node\nRelaxing restrictions allow multiple \"next\" nodes $\\to$ voil√†, a tree\nTypically, \"next\" nodes called *\"children\"*\n(Trees are generalized lists or specialized graphs, depends on point of view)\n\nTrees have *depth:* longest path from *root* to a *leaf*\n*node:* data element\n*edge:* directed connection between nodes\n*path:* sequence of nodes from $n_1$ to $n_k$ such that the sequence follows the child-parent relationship, has *length* $k-1$\n\nevery node has path of length $0$ to itself\nevery node has one path to it from the root\n*depth:* for node $n_i$, the length of path from root to $n_i$\n*height:* for node $n_i$, the length of longest path from $n_i$ to leaf\n*height of tree:* height of root"
    }
  ]
}